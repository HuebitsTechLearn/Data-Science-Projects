here's a runnable version of the core system for your project that you can deploy locally or on cloud (like AWS/GCP):

✅ 1. Install Required Libraries
Run this in your terminal:

bash
Copy
Edit
pip install prophet pandas matplotlib
✅ 2. Forecasting Code (Facebook Prophet)
python
Copy
Edit
import pandas as pd
from prophet import Prophet
import matplotlib.pyplot as plt
import numpy as np

# Simulate sales data for a single SKU
date_range = pd.date_range(start='2022-01-01', end='2024-06-01', freq='D')
np.random.seed(42)
sales_data = pd.DataFrame({
    'ds': date_range,
    'y': np.random.poisson(lam=20, size=len(date_range)) + np.sin(np.arange(len(date_range))/60)*10
})

# Initialize Prophet model
model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)
model.fit(sales_data)

# Forecast next 90 days
future = model.make_future_dataframe(periods=90)
forecast = model.predict(future)

# Plot forecast
fig = model.plot(forecast)
plt.title("Inventory Demand Forecast")
plt.xlabel("Date")
plt.ylabel("Units Sold")
plt.grid(True)
plt.tight_layout()
plt.show()
✅ 3. Optional Enhancements
Add:

Holiday/Promo effects: add_country_holidays(country_name='IN')

Multiple SKUs via looping

Dashboards via Plotly Dash or Flask + Plotly

API integration using Flask or FastAPI

Here's the Flask API code to deploy your Inventory Demand Forecasting system using Facebook Prophet:

python
Copy
Edit
from flask import Flask, request, jsonify
import pandas as pd
from prophet import Prophet
import json

app = Flask(__name__)

@app.route('/forecast', methods=['POST'])
def forecast_sales():
    try:
        # Receive JSON with historical sales data
        data = request.get_json()
        df = pd.DataFrame(data)
        df['ds'] = pd.to_datetime(df['ds'])  # ensure date format

        # Train Prophet model
        model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)
        model.fit(df)

        # Predict next 90 days
        future = model.make_future_dataframe(periods=90)
        forecast = model.predict(future)

        # Return last 30 forecasted records
        result = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(30)
        return jsonify(result.to_dict(orient='records'))

    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True)
✅ How to Use:
Install dependencies:

bash
Copy
Edit
pip install flask prophet pandas
Run the server:

bash
Copy
Edit
python app.py
Send a POST request with historical data in this format:

json
Copy
Edit
[
  {"ds": "2024-01-01", "y": 120},
  {"ds": "2024-01-02", "y": 135},
  ...
]
You’ll get a 30-day forecast with predicted demand values and confidence intervals.



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# --- Configuration ---
# Number of days to look back for forecasting the next day's demand
LOOK_BACK = 30 
# Percentage of data to use for training
TRAIN_SPLIT_RATIO = 0.8
# Number of future days to forecast
FORECAST_HORIZON = 7

# --- 1. Data Generation (Synthetic E-commerce Sales Data) ---
def generate_synthetic_data(num_days=365*3, seasonality_strength=20, trend_strength=0.5):
    """
    Generates synthetic e-commerce sales data with daily fluctuations,
    weekly seasonality, and an overall trend.
    
    Args:
        num_days (int): Total number of days for which to generate data.
        seasonality_strength (float): Amplitude of weekly seasonality.
        trend_strength (float): Linear trend factor.
        
    Returns:
        pandas.DataFrame: DataFrame with 'Date' and 'Demand' columns.
    """
    np.random.seed(42) # for reproducibility
    
    dates = pd.date_range(start='2021-01-01', periods=num_days, freq='D')
    
    # Base demand with some noise
    base_demand = np.random.randint(50, 150, num_days) + 20 * np.sin(np.linspace(0, 3 * np.pi, num_days))
    
    # Add weekly seasonality (e.g., higher sales on weekends)
    day_of_week_effect = np.array([0, 0, 0, 0, 5, 10, 8]) # Mon-Sun effects
    weekly_seasonality = np.array([day_of_week_effect[d.weekday()] for d in dates]) * seasonality_strength / 10
    
    # Add an upward trend
    trend = np.arange(num_days) * trend_strength
    
    # Combine components and add random noise
    demand = (base_demand + weekly_seasonality + trend + np.random.normal(0, 10, num_days)).astype(int)
    demand[demand < 0] = 0 # Ensure demand is non-negative
    
    df = pd.DataFrame({'Date': dates, 'Demand': demand})
    return df

print("Generating synthetic data...")
data = generate_synthetic_data(num_days=365*5, seasonality_strength=25, trend_strength=0.8) # 5 years of data
print("Data head:")
print(data.head())
print("\nData tail:")
print(data.tail())
print(f"\nTotal data points: {len(data)}")

# --- 2. Data Preprocessing for LSTM ---

def create_sequences(data, look_back=1):
    """
    Creates sequences of data for LSTM input.
    Each sequence consists of 'look_back' previous values to predict the next value.
    
    Args:
        data (np.array): Flattened time series data (e.g., scaled demand).
        look_back (int): Number of previous time steps to use as input features.
        
    Returns:
        tuple: (X, y) where X are the input sequences and y are the corresponding target values.
    """
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:(i + look_back), 0])
        y.append(data[i + look_back, 0])
    return np.array(X), np.array(y)

print("\nPreprocessing data for LSTM...")

# Extract the 'Demand' column as a NumPy array
demand_values = data['Demand'].values.reshape(-1, 1)

# Normalize the data to a 0-1 range (important for LSTMs)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_demand = scaler.fit_transform(demand_values)

# Split data into training and testing sets
train_size = int(len(scaled_demand) * TRAIN_SPLIT_RATIO)
train_data, test_data = scaled_demand[0:train_size,:], scaled_demand[train_size:len(scaled_demand),:]

# Create sequences for training and testing
X_train, y_train = create_sequences(train_data, LOOK_BACK)
X_test, y_test = create_sequences(test_data, LOOK_BACK)

# Reshape input to be [samples, time_steps, features] for LSTM
# For univariate series, features = 1
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# --- 3. Build and Train the LSTM Model ---
def build_lstm_model(look_back_steps):
    """
    Builds a Sequential LSTM model for demand forecasting.
    
    Args:
        look_back_steps (int): The number of time steps in each input sequence.
        
    Returns:
        tensorflow.keras.models.Sequential: Compiled LSTM model.
    """
    model = Sequential()
    # First LSTM layer with 50 units, returning sequences for the next LSTM layer
    model.add(LSTM(50, activation='relu', input_shape=(look_back_steps, 1), return_sequences=True))
    model.add(Dropout(0.2)) # Dropout to prevent overfitting
    # Second LSTM layer, not returning sequences as it's the last LSTM layer
    model.add(LSTM(50, activation='relu'))
    model.add(Dropout(0.2))
    # Dense output layer for a single prediction (demand for the next day)
    model.add(Dense(1)) 
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

print("\nBuilding and training LSTM model...")
model = build_lstm_model(LOOK_BACK)

# Define Early Stopping to prevent overfitting and speed up training
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
# Batch size can be tuned (e.g., 32, 64)
# Epochs indicate how many times the model will see the entire training data
history = model.fit(
    X_train, y_train, 
    epochs=100, 
    batch_size=64, 
    validation_split=0.1, # Use a portion of training data for validation
    callbacks=[early_stopping],
    verbose=1
)
print("Model training complete.")

# --- 4. Make Predictions ---
print("\nMaking predictions...")

# Predict on training data
train_predict_scaled = model.predict(X_train)
# Predict on test data
test_predict_scaled = model.predict(X_test)

# Invert predictions to original scale
train_predict = scaler.inverse_transform(train_predict_scaled)
test_predict = scaler.inverse_transform(test_predict_scaled)

# Invert actual values to original scale
y_train_original = scaler.inverse_transform(y_train.reshape(-1, 1))
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

# Calculate RMSE (Root Mean Squared Error)
train_rmse = np.sqrt(mean_squared_error(y_train_original, train_predict))
test_rmse = np.sqrt(mean_squared_error(y_test_original, test_predict))

print(f"Train RMSE: {train_rmse:.2f}")
print(f"Test RMSE: {test_rmse:.2f}")

# --- 5. Visualization ---
print("\nGenerating visualizations...")

# Prepare data for plotting (shift predictions to align with original series)
# Create empty arrays to hold predictions
train_plot = np.empty_like(demand_values)
train_plot[:, :] = np.nan
train_plot[LOOK_BACK:len(train_predict)+LOOK_BACK, :] = train_predict

test_plot = np.empty_like(demand_values)
test_plot[:, :] = np.nan
test_plot[len(train_predict)+(LOOK_BACK*2):len(demand_values), :] = test_predict

plt.figure(figsize=(16, 8))
sns.set_style("whitegrid")

plt.plot(data['Date'], demand_values, label='Actual Demand', color='#1f77b4', linewidth=2)
plt.plot(data['Date'], train_plot, label='Train Prediction', color='#ff7f0e', linestyle='--', linewidth=1.5)
plt.plot(data['Date'], test_plot, label='Test Prediction', color='#2ca02c', linestyle='--', linewidth=1.5)

plt.title('E-commerce Demand Forecasting: Actual vs. Predicted', fontsize=18)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Demand', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot training loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs', fontsize=16)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss (MSE)', fontsize=12)
plt.legend(fontsize=10)
plt.grid(True)
plt.tight_layout()
plt.show()


# --- 6. Future Forecasting (Integration Hook) ---
print("\nForecasting future demand...")

def forecast_future_demand(model, last_n_days_data, scaler, look_back, forecast_horizon):
    """
    Forecasts future demand using the trained LSTM model.
    
    Args:
        model (tf.keras.Model): The trained LSTM model.
        last_n_days_data (np.array): The last 'look_back' days of actual demand data
                                     from the original (unscaled) series.
        scaler (MinMaxScaler): The scaler used for normalization.
        look_back (int): The look-back window used during training.
        forecast_horizon (int): Number of future days to forecast.
        
    Returns:
        np.array: Array of forecasted demand values for the future horizon.
    """
    # Scale the input data
    current_input_scaled = scaler.transform(last_n_days_data.reshape(-1, 1))
    
    future_predictions_scaled = []
    current_batch = current_input_scaled.reshape(1, look_back, 1) # Reshape for LSTM input
    
    for _ in range(forecast_horizon):
        # Get the prediction for the next step
        next_prediction_scaled = model.predict(current_batch)[0, 0]
        future_predictions_scaled.append(next_prediction_scaled)
        
        # Update the input batch: remove the oldest value, add the new prediction
        next_prediction_reshaped = np.array([[next_prediction_scaled]]) # Reshape to (1, 1)
        current_batch = np.append(current_batch[:, 1:, :], next_prediction_reshaped.reshape(1, 1, 1), axis=1)

    # Invert the forecasted values to original scale
    future_predictions = scaler.inverse_transform(np.array(future_predictions_scaled).reshape(-1, 1))
    return future_predictions.flatten()

# Get the last `LOOK_BACK` days of actual demand data
last_look_back_data = demand_values[-LOOK_BACK:]

# Generate future forecasts
future_forecasts = forecast_future_demand(model, last_look_back_data, scaler, LOOK_BACK, FORECAST_HORIZON)

print(f"\nForecasted demand for the next {FORECAST_HORIZON} days:")
for i, forecast in enumerate(future_forecasts):
    print(f"Day {i+1}: {forecast:.2f}")

# Plotting future forecasts
future_dates = pd.date_range(start=data['Date'].iloc[-1] + pd.Timedelta(days=1), 
                             periods=FORECAST_HORIZON, freq='D')

plt.figure(figsize=(12, 6))
plt.plot(data['Date'][-90:], demand_values[-90:], label='Recent Actual Demand', color='blue') # Last 90 days
plt.plot(future_dates, future_forecasts, label=f'Forecasted Demand (next {FORECAST_HORIZON} days)', color='red', linestyle='--')

plt.title('E-commerce Demand Forecasting: Recent Actuals and Future Forecasts', fontsize=16)
plt.xlabel('Date', fontsize=14)
plt.ylabel('Demand', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 7. Model Saving and Loading (Integration Hook) ---
print("\nSaving and loading model for integration...")

# Save the trained model
model_filename = 'ecommerce_demand_forecast_lstm.h5'
model.save(model_filename)
print(f"Model saved as '{model_filename}'")

# In a real application, you would load the model when needed
# For demonstration, let's load it immediately
loaded_model = load_model(model_filename)
print(f"Model loaded from '{model_filename}'")

# Verify loaded model can make predictions
# You would use the 'last_look_back_data' from your current real-time data
test_input_for_loaded_model_scaled = scaler.transform(last_look_back_data.reshape(-1, 1))
test_input_for_loaded_model_reshaped = test_input_for_loaded_model_scaled.reshape(1, LOOK_BACK, 1)

loaded_model_prediction_scaled = loaded_model.predict(test_input_for_loaded_model_reshaped)
loaded_model_prediction = scaler.inverse_transform(loaded_model_prediction_scaled)[0, 0]

print(f"Prediction from loaded model for next day: {loaded_model_prediction:.2f}")

# --- 8. Integration Hooks & Further Considerations ---

print("\n--- Integration Hooks & Further Considerations ---")

# a) API Endpoint for Real-time Forecasting:
print("\nTo integrate into an e-commerce platform, you could create a REST API endpoint.")
print("When a request comes in for a forecast:")
print("1. Fetch the latest N (LOOK_BACK) days of sales data for the specific product.")
print("2. Preprocess this data using the SAME scaler (saved during training).")
print("3. Reshape the data (samples, time_steps, features).")
print("4. Call loaded_model.predict() to get the forecast.")
print("5. Inverse transform the prediction to get the actual demand value.")
print("6. Return the forecast in a JSON response.")

# Example pseudo-code for an API endpoint (using Flask/FastAPI)
# from flask import Flask, request, jsonify
# import joblib # to save/load scaler
#
# app = Flask(__name__)
#
# # Load model and scaler once when the application starts
# try:
#     GLOBAL_LOADED_MODEL = load_model('ecommerce_demand_forecast_lstm.h5')
#     GLOBAL_SCALER = joblib.load('min_max_scaler.pkl') # Save your scaler during training
# except Exception as e:
#     print(f"Error loading model/scaler: {e}")
#     GLOBAL_LOADED_MODEL = None
#     GLOBAL_SCALER = None
#
# @app.route('/forecast_demand', methods=['POST'])
# def forecast_demand():
#     if GLOBAL_LOADED_MODEL is None or GLOBAL_SCALER is None:
#         return jsonify({'error': 'Model or scaler not loaded'}), 500
#
#     try:
#         # Expecting a JSON array of historical demand values
#         data = request.json.get('historical_demand', [])
#         if len(data) != LOOK_BACK:
#             return jsonify({'error': f'Expected {LOOK_BACK} historical demand values'}), 400
#
#         historical_data_np = np.array(data).reshape(-1, 1)
#         scaled_input = GLOBAL_SCALER.transform(historical_data_np)
#         reshaped_input = scaled_input.reshape(1, LOOK_BACK, 1)
#
#         prediction_scaled = GLOBAL_LOADED_MODEL.predict(reshaped_input)
#         prediction_original = GLOBAL_SCALER.inverse_transform(prediction_scaled)[0, 0]
#
#         return jsonify({'predicted_demand': float(prediction_original)}), 200
#     except Exception as e:
#         return jsonify({'error': str(e)}), 400
#
# if __name__ == '__main__':
#     # In your training script, save the scaler:
#     # joblib.dump(scaler, 'min_max_scaler.pkl')
#     # app.run(debug=True, port=5000)
print("--- End of Script ---")
